{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8460: Traitement automatique de la langue naturelle\n",
    "\n",
    "### TP2: Autocomplétion et génération de phrases avec des modèles de langue n-grammes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification de l'équipe:\n",
    "\n",
    "### Groupe de laboratoire: \n",
    "\n",
    "### Equipe numéro : \n",
    "\n",
    "### Membres: \n",
    "\n",
    "- membre 1 (% de contribution, nature de la contribution)\n",
    "- membre 2 (% de contribution, nature de la contribution)\n",
    "- membre 3 (% de contribution, nature de la contribution)\n",
    "\n",
    "* nature de la contribution: Décrivez brièvement ce qui a été fait par chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "\n",
    "Ce deuxième travail pratique sera dédié à l'utilisation des n-grammes en traitement du langage naturel (NLP). Les n-grammes sont des séquences de n mots consécutifs dans un texte, et ils sont présent dans de nombreuses applications du NLP.\n",
    "\n",
    "Au cours de ce laboratoire, nous allons explorer comment les n-grammes sont utilisés pour des tâches telles que la prédiction de mots, l'autocomplétion et la génération de texte. Voici quelques exemples concrets de leur utilisation :\n",
    "\n",
    "Prédiction de Mots : Les n-grammes sont utilisés pour prédire le mot suivant étant donné le contexte précédent. Par exemple, dans la phrase \"Je pense donc je ____\", un modèle de bigramme pourrait suggérer \"suis\" comme mot suivant, basé sur des observations antérieures.\n",
    "\n",
    "Autocomplétion : Les moteurs de recherche utilisent les n-grammes pour proposer des suggestions de recherche lorsque vous commencez à taper une phrase. Ils se basent sur les préfixes du mot courant et sur les phrases écrites par l'utilisateur.\n",
    "\n",
    "Génération de Texte : Les n-grammes peuvent être utilisés pour générer automatiquement du texte. Ils aident un modèle à prédire les mots suivants en fonction des probabilités des mots pouvant être générés selon le contexte.\n",
    "\n",
    "Vous étudierez aussi une manière d'évaluer la qualité des modèles de langage génératifs en utilisant la perplexité. La perplexité est une métrique qui évalue la confiance avec laquelle un modèle est capable de prédire une séquence de mots dans un texte. Plus la perplexité est faible, plus le modèle est capable de prédire avec précision. Autrement dit, la perplexité est une mesure de la \"surprise\" d'un modèle lorsqu'il est exposé à un nouvel ensemble de mots. Nous utiliserons cette métrique pour évaluer nos modèles basés sur les n-grammes.\n",
    "\n",
    "\n",
    "**NOTE: seulement les librairies standards de python (et numpy) sont permises ainsi que celles déjà importées le notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1.  Chargement et pré-traitement des données (12 points)\n",
    "\n",
    "<a name='1.1'></a>\n",
    "### 1.1 Chargement des données (1 point)\n",
    "\n",
    "Les données que vous allez utiliser dans ce travail sont contenues dans le fichier [trump.txt](./trump.txt)\n",
    "\n",
    "Lisez le contenu de ce fichier et stockez-le dans une variable data.\n",
    "\n",
    "Affichez ensuite les 300 premiers caractères\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you very much.\n",
      "We had an amazing convention.\n",
      "That was one of the best.\n",
      "I think it was one of the best ever.\n",
      "In terms -- in terms of enthusiasm, in terms of I think what it represents, getting our word out.\n",
      "Ivanka was incredible last night.\n",
      "She did an incredible job.\n",
      "And so many of the speakers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"trump.txt\"\n",
    "\n",
    "data = open(filename, 'r').read()\n",
    "print(data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2  Segmentation (2 points)\n",
    "\n",
    "Pré-traitez les données en suivant les étapes suivantes:\n",
    "\n",
    "1. Enlever les majuscules.\n",
    "2. Remplacer les \"\\n\" par des espaces\n",
    "3. Séparer les données en phrases en utilisant les délimiteurs suivants `.`, `?` et `!` comme séparateur.\n",
    "4. Enlever les signes de ponctuation (Attention de garder les espaces).\n",
    "5. Enlever les phrases vides.\n",
    "6. Segmenter les phrases avec la fonction nltk.word_tokenize()\n",
    "\n",
    "Utilisez ensuite votre fonction pour prétraiter le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def preprocess(data):\n",
    "    data = data.lower()\n",
    "    data = data.replace('\\n', ' ')\n",
    "    data = re.split(r'[.?!]', data)\n",
    "    data = [re.sub(r'[^\\w\\s]', '', s) for s in data]\n",
    "    data = [s for s in data if len(s) > 0]\n",
    "    tokens = [nltk.word_tokenize(s) for s in data]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "x = \"Cats are independent.\\nDogs are faithful.\"\n",
    "preprocess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "###  1.3 Création d'ensembles d'entraînement et de test (1 point)\n",
    "\n",
    "Échantilloner de manière **aléatoire** 80% des données pour l'ensemble d'entrainement. Garder 20% pour l'ensemble de test. Utilisez la fonction train_test_split de sklearn. Stocker les résultats dans des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "### 1.4 Construction du vocabulaire (4 points)\n",
    "\n",
    "Comme dans le TP1, construisez un vocabulaire à partir des données d'entraînement. Vous pouvez reprendre votre code du TP1.\n",
    "\n",
    "Complétez la fonction **build_voc** qui retourne une liste de jetons qui sont présents au moins n fois (threshold passé en paramètre) dans la liste d'exemples (également passée en paramètre). Vous pouvez utiliser la classe collections.Counter.\n",
    "\n",
    "Ensuite, appelez cette fonction pour construire votre vocabulaire à partir de l'ensemble d'entraînement **en utilisant threshold=2**. Imprimez la taille du vocabulaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def build_voc(documents, threshold):\n",
    "    tokens = []\n",
    "    for document in documents:\n",
    "        for token in document:\n",
    "            tokens.append(token)\n",
    "\n",
    "    counter = Counter(tokens)\n",
    "    vocabulary = []\n",
    "    for token, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "    return set(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5608\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_voc(train, 2)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "### 1.5 Mots hors vocabulaire (4 points)\n",
    "\n",
    "Si votre modèle réalise de l'autocomplétion, mais qu'il rencontre un mot qu'il n'a jamais vu lors de l'entraînement, le modèle ne pourra donc pas prédire le mot suivant car il n'y a pas d'occurrence pour le mot actuel.\n",
    "\n",
    "Ces mots sont appelés les mots hors vocabulaire (Out of Vocabulary) <b>OOV</b>.\n",
    "Le pourcentage de mots inconnus dans l'ensemble de test est appelé le taux de mots <b> OOV </b>.\n",
    "\n",
    "Pour gérer les mots inconnus lors de la prédiction, utilisez un jeton spécial 'unk' pour représenter tous les mots inconnus. Plus spécifiquement, la technique que vous utiliserez sera la suivante:\n",
    "\n",
    "Complétez la fonction replace_oov qui convertit tous les mots qui ne font pas partie du vocabulaire en jeton '\\<unk\\>'.\n",
    "\n",
    "Appelez ensuite votre fonction sur votre corpus d'entraînement et de test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov(tokenized_sentences, voc):\n",
    "    modified_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        # add a list of size of len(sentence)\n",
    "        modified_sentence = []\n",
    "        for token in sentence:\n",
    "            if token not in voc:\n",
    "                modified_sentence.append('<unk>')\n",
    "            else:\n",
    "                modified_sentence.append(token)\n",
    "        modified_sentences.append(modified_sentence)\n",
    "    return modified_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase initiale:\n",
      "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
      "Phrase segmentée avec'<unk>':\n",
      "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]]\n",
    "vocabulary = build_voc([[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]], 2)\n",
    "tmp_replaced_tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
    "print(f\"Phrase initiale:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"Phrase segmentée avec'<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "```CPP\n",
    "Phrase initiale:\n",
    "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
    "Phrase segmentée avec '<unk>':\n",
    "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary = build_voc(train, 2)\n",
    "test_vocabulary = build_voc(test, 2)\n",
    "\n",
    "train = replace_oov(train, train_vocabulary)\n",
    "test = replace_oov(test, test_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Modèles de langue n-gramme (18 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, vous développerez un modèle de langue n-grammes. Nous allons utiliser la formule: \n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "- La fonction $C(\\cdots)$ représente le nombre d'occurrences de la séquence donnée.\n",
    "- $\\hat{P}$ signifie l'estimation de $P$.\n",
    "\n",
    "Vous pouvez estimer cette probabilité en comptant les occurrences de ces séquences de mots dans les données d'entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 Fréquence des n-grammes (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Vous allez commencer par mettre en œuvre une fonction qui calcule la fréquence des n-grammes pour un nombre arbitraire $n$.\n",
    "\n",
    "Vous devez pré-traiter la phrase en ajoutant $n$ marqueurs de début de phrase \"\\<s\\>\" pour indiquer le commencement de la phrase.\n",
    "\n",
    "- Par exemple, dans un modèle bigramme (N=2), la séquence devrait commencer avec deux jetons de début \"\\<s\\>\\<s\\>\". Ainsi, si la phrase est \"J'aime la nourriture\", modifiez-la pour devenir \"\\<s\\>\\<s\\> J'aime la nourriture\".\n",
    "- Ajoutez aussi un jeton de fin \"\\<e\\>\" pour que le modèle puisse prédire quand terminer une phrase.\n",
    "    \n",
    "\n",
    "Dans cette implémentation, vous devez stocker les occurrences des n-grammes sous forme de dictionnaire.\n",
    "\n",
    "- La clé de chaque paire clé-valeur dans le dictionnaire est un tuple de n mots (et non une liste).\n",
    "- La valeur dans la paire clé-valeur est le nombre d'occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
    "    n_grams = {}\n",
    "    for sentence in data:\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = tuple(sentence[i:i+n])\n",
    "            if n_gram not in n_grams:\n",
    "                n_grams[n_gram] = 1\n",
    "            else:\n",
    "                n_grams[n_gram] += 1\n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrammes:\n",
      "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
      "Bigrammes:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "print(\"Unigrammes:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bigrammes:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue:\n",
    "\n",
    "```CPP\n",
    "Unigrammes:\n",
    "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
    "Bigrammes:\n",
    "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 Estimé du maximum de vraisemblance MLE (4 points)\n",
    "\n",
    "#### 2.2.1 Calcul de probabilité pour un mot (3 points)\n",
    "\n",
    "\n",
    "Ensuite, estimez la probabilité d'un mot étant donnés les 'n' mots précédents avec les fréquences obtenues.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2}$$ \n",
    "\n",
    "\n",
    "La fonction prend en entrée: \n",
    "\n",
    "- word : le mot dont on veut estimer la probabilité\n",
    "- previous_n_gram : le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = 0\n",
    "    if (previous_n_gram in n_gram_counts):\n",
    "        previous_n_gram_count = n_gram_counts[previous_n_gram]\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = 0\n",
    "    if n_plus1_gram in n_plus1_gram_counts:\n",
    "        n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram]\n",
    "\n",
    "    probability = n_plus1_gram_count / previous_n_gram_count\n",
    "    return probability    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Quel est le problème de cette fonction? Quelle embûche pourrait-on rencontrer? Répondre avec un exemple. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3  Lissage add-k (4 points)\n",
    "\n",
    "Vous allez maintenant modifier votre fonction précédente en utilisant le lissage add-k.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "Recodez la fonction au numéro 2.2 en ajoutant une constante de lissage $k$ and la taille du vocabulaire en paramètres supplémentaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = 0\n",
    "    if (previous_n_gram in n_gram_counts):\n",
    "        previous_n_gram_count = n_gram_counts[previous_n_gram]\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = 0\n",
    "    if n_plus1_gram in n_plus1_gram_counts:\n",
    "        n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram]\n",
    "    probability = (n_plus1_gram_count + k) / (previous_n_gram_count + (k * vocabulary_size))\n",
    "    return probability   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "tmp_prob = estimate_probability_smoothing(\"have\", ['<s>', 'i'], bigram_counts, trigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\" La probabilité de 'have' étant donné le mot précédent 'i' est: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 Calcul des probabilités des n-grammes (7 points)\n",
    "\n",
    "#### 2.4.1. Estimation des probabilités (4 points) \n",
    "Complétez la fonction estimate_probabilities qui calcule pour chaque mot du vocabulaire la probabilité d'être généré en utilisant la fonction avec lissage add-k. \n",
    "\n",
    "N'oubliez pas d'ajouter le jetons spécial \"\\<e\\>\" au vocabulaire\n",
    "\n",
    "Cette fonction prends en entrée:\n",
    "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
    "- vocabulary: le vocabulaire\n",
    "- k: la constante de lissage\n",
    "\n",
    "La fonction retourne un dictionnaire ayant pour clés tous les mots du vocabulaire ainsi que leur probabilité d'être générés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    probability_dict = {}\n",
    "    vocabulary.append('<e>')\n",
    "    for word in vocabulary:\n",
    "        probability_dict[word] = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k)\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'have': 0.1111111111111111,\n",
       " 'mouse': 0.2222222222222222,\n",
       " 'this': 0.1111111111111111,\n",
       " 'i': 0.1111111111111111,\n",
       " 'likes': 0.1111111111111111,\n",
       " 'cats': 0.1111111111111111,\n",
       " 'a': 0.1111111111111111,\n",
       " '<e>': 0.1111111111111111}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "{'likes': 0.1111111111111111,\n",
    " 'have': 0.1111111111111111,\n",
    " 'this': 0.1111111111111111,\n",
    " 'i': 0.1111111111111111,\n",
    " 'mouse': 0.2222222222222222,\n",
    " 'a': 0.1111111111111111,\n",
    " 'cats': 0.1111111111111111,\n",
    " '<e>': 0.1111111111111111}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Probabilités étant donné un contexte (3 points)\n",
    "\n",
    "Affichez maintenant les probabilités des tri-grammes étant donné le context \"i will\" en utilisant les données d'entraînement . N'affichez que les 10 mots les plus probables en ordre décroissant de probabilité. Utilisez K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tell', 0.008265856950067476)\n",
      "('fight', 0.005566801619433198)\n",
      "('fix', 0.0052294197031039135)\n",
      "('be', 0.004554655870445344)\n",
      "('never', 0.004048582995951417)\n",
      "('say', 0.00354251012145749)\n",
      "('not', 0.0021929824561403508)\n",
      "('ask', 0.0020242914979757085)\n",
      "('also', 0.0016869095816464238)\n",
      "('work', 0.0016869095816464238)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "previous_n_gram = [\"i\", \"will\"]\n",
    "vocabulary = list(build_voc(train, 0))\n",
    "bigram_counts = count_n_grams(train, 2)\n",
    "trigram_counts = count_n_grams(train, 3)\n",
    "probabilities = estimate_probabilities(previous_n_gram, bigram_counts, trigram_counts, vocabulary, k=1)\n",
    "# print(sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "for (prob) in sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "[('tell', 0.0070981916511745815),\n",
    " ('fix', 0.005239141456819334),\n",
    " ('fight', 0.005239141456819334),\n",
    " ('be', 0.0050701368936961295),\n",
    " ('never', 0.004225114078080108),\n",
    " ('say', 0.003718100388710495),\n",
    " ('not', 0.0025350684468480648),\n",
    " ('ask', 0.0023660638837248605),\n",
    " ('also', 0.0018590501943552475),\n",
    " ('work', 0.001521041068108839)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Perplexité (15 points)\n",
    "\n",
    "Dans cette section, vous allez générer le score de perplexité pour évaluer votre modèle sur l'ensemble de test.\n",
    "\n",
    "Pour calculer le score de perplexité d'une phrase sur un modèle n-gramme, utilisez :\n",
    "\n",
    "$$PP(W) =\\sqrt[N]{ \\prod_{t=1}^{N} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n",
    "\n",
    "où N = le nombre de jeton dans la phrases incluant le jeton \\<e\\>\n",
    "et P = la probabilité de générer le jeton $w_t$\n",
    "\n",
    "Plus les probabilités sont élevées, plus la perplexité sera basse. \n",
    "\n",
    "<a name='3.1'></a>\n",
    "### 3.1. Calcul de la perplexité (4 points)\n",
    "Complétez la fonction `calculate_perplexity`, qui pour une phrase donnée, nous donne le score de perplexité. Cette fonction prend en entrée:\n",
    "\n",
    "\n",
    "- sentence: La phrase pour laquelle vous devez calculer la perplexité\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
    "- vocabulary_size: la taille du vocabulaire\n",
    "- k: la constante de lissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    sentence = sentence + [\"<e>\"]\n",
    "    N = len(sentence)\n",
    "    product = 1\n",
    "\n",
    "    for i in range(N):\n",
    "        word = sentence[i]\n",
    "        previous_n_gram = sentence[i-1]\n",
    "        probability = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "        product = product * 1/probability\n",
    "    perplexity = product**(1/N)\n",
    "    return perplexity\n",
    "    # n = len(list(n_gram_counts.keys())[0])\n",
    "    # # new_sentence = [\"<s>\"] * n_gram_size\n",
    "    # # new_sentence += sentence\n",
    "    # # new_sentence.append(\"<e>\")\n",
    "    # # new_sentence = tuple(new_sentence)\n",
    "    # new_sentence = tuple(sentence)\n",
    "\n",
    "    # N = len(new_sentence)\n",
    "    # product = 1\n",
    "    # for t in range(n, N):\n",
    "    #     n_gram = new_sentence[t-n:t]\n",
    "    #     word = new_sentence[t]\n",
    "    #     probability = estimate_probability_smoothing(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "    #     product = product * 1/probability\n",
    "    # perplexity = product**(1/N)\n",
    "    # return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité de la première phrase: 5.5961\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "\n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexité de la première phrase: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2. Perplexité sur une phrase d'entraînement (4 points)\n",
    "Calculez et affichez la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes à l'aide de votre fonction `calculate_perplexity` définie plus haut sur la première phrase de votre corpus d'entraînement. Utilisez K=0.01 ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité bigramme: 2311.1495\n",
      "Perplexité trigramme: 5609.0000\n",
      "Perplexité quadrigramme: 5609.0000\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_voc(train, 2)\n",
    "unigram_counts = count_n_grams(train, 1)\n",
    "bigram_counts = count_n_grams(train, 2)\n",
    "trigram_counts = count_n_grams(train, 3)\n",
    "quadrigram_counts = count_n_grams(train, 4)\n",
    "\n",
    "perplexity_bigram = calculate_perplexity(test[0], unigram_counts, bigram_counts, len(vocabulary), k=1)\n",
    "perplexity_trigram = calculate_perplexity(test[0], bigram_counts, trigram_counts, len(vocabulary), k=1)\n",
    "perplexity_quadgram = calculate_perplexity(test[0], trigram_counts, quadrigram_counts, len(vocabulary), k=1)\n",
    "\n",
    "print(f\"Perplexité bigramme: {perplexity_bigram:.4f}\")\n",
    "print(f\"Perplexité trigramme: {perplexity_trigram:.4f}\")\n",
    "print(f\"Perplexité quadrigramme: {perplexity_quadgram:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3. Perplexité du corpus de test (7 points)\n",
    "\n",
    "#### 3.3.1. Vous pouvez maintenant calculer et afficher la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes sur votre corpus de test. K=1 ici. (4 points)\n",
    "\n",
    "Pour calculer la perplexité d'un corpus de *m* phrases, il suffit de suivre la formule suivante : \n",
    "\n",
    "Soit $N$ le nombre total de jetons dans le corpus de test C et $N_i$ le nombre de jetons dans la phrase i.\n",
    "\n",
    "$$Perplexity(C) = \\Big(\\frac{1}{P(s_1, ..., s_m)}\\Big)^{1/N}$$ \n",
    "$$P(s_1, ..., s_m) = \\prod_{i=1}^{m} p(s_i)$$\n",
    "$$p(s_i) = \\prod_{t=1}^{N_i} \\hat{P}(w_t | w_{t-n} \\cdots w_{t-1})$$\n",
    "\n",
    "Puisqu'il s'agit d'un multiplication de probabilités (situées entre 0 et 1), le produit devient nul très rapidement. C'est pourquoi il est plus efficace d'effectuer une transformation vers un espace logarithmique pour transformer les multiplications en addition. Cela donne ainsi la formule suivante:\n",
    "\n",
    "$$LogPerplexity(C) = 2^{-\\frac{1}{N} \\sum_{k=1}^{m} log_{2} \\; p(s_k)}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_corpus(corpus, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    N = 0\n",
    "    product = 1\n",
    "\n",
    "    for sentence in corpus:\n",
    "        N += len(sentence) + 1\n",
    "        product *= calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "    perplexity = product**(1/N)\n",
    "\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité du corpus de test: 7.7089\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts = {('<s>', 'quick'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
    "n_plus1_gram_counts = { ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
    "\n",
    "train_corpus = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
    "n_gram_counts = {('<s>', '<s>'): 2, ('<s>', 'the'): 1, ('<s>', 'jumps'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', '<e>'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
    "n_plus1_gram_counts = {('<s>', '<s>', '<s>', ): 2, ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1,  ('<s>', '<s>', 'jumps', ): 1, ('<s>', 'jumps', 'over'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', '<e>'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
    "\n",
    "vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"<e>\"]\n",
    "\n",
    "N = 5\n",
    "V = len(vocabulary)\n",
    "\n",
    "test_corpus = [[\"the\", \"fox\"], [\"jumps\"]]\n",
    "\n",
    "# Complétez le calcul de la perplexité avec k=1\n",
    "perplexity = calculate_perplexity_corpus(test_corpus, n_gram_counts, n_plus1_gram_counts, V, k=1)\n",
    "print(f\"Perplexité du corpus de test: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sortie attendue\n",
    "\n",
    "    Perplexité du corpus de test:  7.708920690856638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculez mainenant la perplexité de votre corpus de test\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Les perplexités attendues peuvent sembler contre-intuitives.  Comparez-les aux perplexités obtenues sur l'ensemble d'entrainement pour les mêmes modèles. Comment expliquez-vous ces résultats et quelle est votre conclusion ?  (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4. Construction d'un modèle d'auto-complétion (15 points)\n",
    "\n",
    "Dans cette dernière partie, vous allez utiliser les modèles n-grammes construits aux numéros précédents afin de faire un modèle d'autocomplétion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 Suggestion d'un mot à partir d'un préfixe (5 points)\n",
    "\n",
    "\n",
    "La première étape sera de construire une fonction qui suggère un mot à partir des premiers caractères entrés par un utilisateur, considérant un seul type de n-gramme.  \n",
    "\n",
    "Complétez la fonction `suggest_word` qui calcule les probabilités pour tous les mots suivants possibles et suggère le mot le plus probable. Comme contrainte supplémentaire, le mot suggéré doit commencer avec le préfixe passé en paramètre. Utilisez vos fonctions provenant du numéro 2. (Modèle n-gramme de mots) pour faire vos prédictions.\n",
    "\n",
    "Cette fonction prends en paramètre:\n",
    "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
    "- vocabulary_size: la taille du vocabulaire\n",
    "- k: la constante de lissage\n",
    "- prefixe: Le début du mot que l'on veut prédire\n",
    "\n",
    "Elle retourne le mot le plus probable avec la probabilité associée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, prefixe=\"\"):\n",
    "    \n",
    "    return \"word\", 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " avec les mots précédents 'i have',\n",
      "\t le mot suggéré est `word` avec la probabilité 0.0000\n",
      "\n",
      "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
      "\t le mot suggéré est : `word` avec une probabilité de 0.0000\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"have\"]\n",
    "tmp_suggest1 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\" avec les mots précédents 'i have',\\n\\t le mot suggéré est `{tmp_suggest1[0]}` avec la probabilité {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "tmp_starts_with = 'm'\n",
    "tmp_suggest2 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, prefixe=tmp_starts_with)\n",
    "print(f\"avec les mots précédents 'i have', et une suggestion qui commence par `{tmp_starts_with}`\\n\\t le mot suggéré est : `{tmp_suggest2[0]}` avec une probabilité de {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "avec les mots précédents 'i have',\n",
    "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
    "\n",
    "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
    "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 Suggestions multiples (5 points)\n",
    "\n",
    "Afin de suggérer plusieurs mots à l'utilisateur, une stratégie que l'on peut utiliser est de retourner un ensemble de mots suggérés par plusieurs types de modèles n-grammes.\n",
    "\n",
    "En utilisant la fonction `suggest_word` du numéro précédent, complétez la fonction `get_suggestions` qui retourne les suggestions des modèles n-grammes passés en paramètre. Vous devrez aussi enlever les doublons dans les suggestions s'il y en a, et ordonner la liste des suggestions en commençant par le mot ayant la probabilité la plus élevée.\n",
    "\n",
    "La fonction get_suggestions prends en paramètres:\n",
    "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts_list: une liste de n-grammes dans l'ordre suivant [unigrammes, bigrammes, trigrammes, quadrigrammes, ...]\n",
    "- vocabulary_size: la taille du vocabulaire\n",
    "- k: la constante de lissage (entre 0 et 1)\n",
    "- prefixe: Le début du mot que l'on veut prédire, \"\" si au aucun préfixe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, prefixe=\"\"):\n",
    "\n",
    "    return [\"word1\", \"word2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etant donné les mots i have, je suggère :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['word1', 'word2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test \n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"have\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"Etant donné les mots i have, je suggère :\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP \n",
    "Etant donné les mots i have, je suggère :\n",
    "['a']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.3'></a>\n",
    "### 4.3 Autocomplétion (5 points)\n",
    "\n",
    "Il est maintenant temps de combiner vos fonctions afin de créer le modèle d'autocomplétion. En utilisant le jeu de données d'entraînement, calculez la fréquence des n-grammes allant de 1 à 5 et utilisez la fonction *get_suggestions* afin de suggérer des mots. Vous devrez être en mesure de toujours suggérer des mots à partir du dernier mot entré par l'utilisateur.\n",
    "\n",
    "Complétez la fonction *update_suggestions*:\n",
    "- la variable texte_actuel contient tout le texte entré par l'utilisateur\n",
    "- la variable top_suggestions contient les suggestions qui seront proposées\n",
    "\n",
    "Vous devrez changer le contenu de la variable top_suggestions pour qu'elle contienne les suggestions des n-grammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\TP A2023\\INF8460\\INF8460_TP2\\TP2_inf8460_A23_Andy.ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP2/TP2_inf8460_A23_Andy.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mipywidgets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mwidgets\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP2/TP2_inf8460_A23_Andy.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/TP%20A2023/INF8460/INF8460_TP2/TP2_inf8460_A23_Andy.ipynb#Y122sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m text_input \u001b[39m=\u001b[39m widgets\u001b[39m.\u001b[39mText(placeholder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEntrez votre text ici...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "text_input = widgets.Text(placeholder=\"Entrez votre text ici...\")\n",
    "\n",
    "suggestions_label = widgets.Label(value=\"Suggestions: \")\n",
    "\n",
    "def update_suggestions(change):\n",
    "    texte_actuel = change[\"new\"]\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    top_suggestions = [\"word1\", \"word2\"]\n",
    "    suggestions_label.value = \"Suggestions: \" + \", \".join(top_suggestions)\n",
    "\n",
    "text_input.observe(update_suggestions, names=\"value\")\n",
    "\n",
    "display(text_input)\n",
    "display(suggestions_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5. Modèle de génération de phrases (30 points)\n",
    "\n",
    "Dans cette partie vous allez construire un modèle de génération de phrases en utilisant les n-grammes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dans la cadre d'un modèle de génération de phrases, indiquez pourquoi la stratégie de suggestion des mots en 4. ne peut pas fonctionner ? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.1'></a>\n",
    "\n",
    "### 5.1 Génération stochastique de mots (5 points)\n",
    "\n",
    "Recodez la fonction suggest_word afin d'utiliser une suggestion stochastique. Autrement dit, au lieu de retourner le mot le plus probable, vous devrez générez le mot suivant selon sa probabilité.\n",
    "\n",
    "Par exemple si le mot 'like' a la probabilité 0.25 d'être généré, alors il sera retourné 25% du temps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def suggest_word_with_probs(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "\n",
    "\n",
    "    return 'word', 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.2'></a>\n",
    "### 5.2 Générations de phrases (10 points)\n",
    "\n",
    "#### 5.2.1. Génération stochastique (4 points)\n",
    "Complétez maintenant la fonction `generate_sentence` qui génère une phrase longue de n_words en appelant votre nouvelle fonction `suggest_words_with_probs`. La génération doit s'arrêter si le modèle génère un jeton de fin de phrase.\n",
    "\n",
    "Il ne faut pas oublier d'initialiser les phrases à générer avec le bon nombre de jetons de début de phrase (`<s>`). Par exemple, s'il s'agit d'un modèle bigramme, il faudra initialiser la phrase à [`<s>`]. S'il s'agit d'un modèle trigramme, il faudra initialiser la phrase à [`<s>`, `<s>`]. Vous pouvez trouver la taille du contexte à l'aide de l'expression suivante `len(next(iter(n_gram_counts)))`.\n",
    "\n",
    "Ensuite, il faudra passer à la fonction `suggest_word` les `n` derniers mots générés où `n` correspond à la taille du contexte.\n",
    "Finalement, il faudra arrêter la génération si le jeton généré est le jeton de fin (`<e>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.0001):\n",
    "\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Test sur des n-grammes (2 points)\n",
    "Testez ensuite votre fonction avec des trigrammes et des 5-grammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Avec k=1.0, que se passe-t-il avec les phrases générées et quelle en est la raison principale ? Que pouvez-vous faire pour améliorer la situation? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4.  Quels sont les problèmes si la constante k a une valeur trop petite, voir 0?  (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.3'></a>\n",
    "### 5.3. Amélioration de la génération stochastique de mots (12 points)\n",
    "\n",
    "#### 5.3.1. Amélioration stochastique \n",
    "\n",
    "Comme vous avez pu l'observer, la génération stochastique, bien qu'elle soit efficace pour générer des phrases différentes, a tendance à ne pas générer des phrases toujours cohérentes. Proposez une amélioration de la méthode `suggest_word` que vous implémenterez dans la méthode `suggest_word_new` permettant de générer des phrases plus cohérentes. \n",
    "\n",
    "##### a) Décrivez votre méthode dans la cellule suivante (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Implémentez la méthode proposée (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def suggest_word_new(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2. Génération améliorée (2 points)\n",
    "Recodez maintenant la fonction `generate_sentence_new` pour appeler votre nouvelle méthode `suggest_word_new`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_new(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.001):\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3. Test sur des n-grammes (2 points)\n",
    "Testez ensuite votre fonction avec des 3-grammes et des 5-grammes et validez que les phrases sont plus cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIVRABLES:\n",
    "Vous devez remettre sur Moodle, avant la date d'échéance, un zip contenant les fichiers suivants :\n",
    "\n",
    "1-\tLe code : Vous devez compléter le squelette inf8460_tp2.ipynb sous le nom   equipe_i_inf8460_TP2.ipynb (i = votre numéro d’équipe). Indiquez vos noms et matricules au début du notebook. Ce notebook doit contenir les fonctionnalités requises avec des commentaires appropriés. Le code doit être exécutable sans erreur et accompagné de commentaires appropriés de manière à expliquer les différentes fonctions. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ; \n",
    "\n",
    "2-\tUn fichier pdf représentant votre notebook complètement exécuté sous format pdf. \n",
    "Pour créer le fichier cliquez sur File > Download as > PDF via LaTeX (.pdf). Assurez-vous que le PDF est entièrement lisible.\n",
    "\n",
    "\n",
    "## EVALUATION \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "\n",
    "1. Exécution correcte du code\n",
    "2. Qualité du code (noms significatifs, structure, gestion d’exception, etc.) avec, entre autres, les recommandations suivantes:\n",
    "    - Il ne devrait pas y avoir de duplication de code. Utilisez des fonctions pour garder votre code modulaire\n",
    "    - Votre code devrait être optimisé: un code trop lent entraînera une perte de points\n",
    "3. Lisibilité du code (Commentaires clairs et informatifs au besoin)\n",
    "4. Performance/sortie attendue des modèles\n",
    "5. Réponses correctes/sensées aux questions de réflexion ou d'analyse\n",
    " "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
