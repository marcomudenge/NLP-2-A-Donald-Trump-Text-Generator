{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afz5ucwvRf_k"
      },
      "source": [
        "# Automatic natural language processing\n",
        "\n",
        "### Autocompletion and sentence generation with n-gram language models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t-x0LjjYRf_n"
      },
      "source": [
        "## The team\n",
        "\n",
        "- Grover-Brando Tovar Oblitas\n",
        "- Andy Chen\n",
        "- Marco Mudenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOwVeGW2Rf_o"
      },
      "source": [
        "## Description:\n",
        "\n",
        "This second notebook will be dedicated to the use of n-grams in natural language processing (NLP). N-grams are sequences of n consecutive words in text, and they are present in many applications of NLP.\n",
        "\n",
        "During this lab, we will explore how n-grams are used for tasks such as word prediction, autocompletion, and text generation. Here are some concrete examples of their use:\n",
        "\n",
        "Word Prediction: N-grams are used to predict the next word given the previous context. For example, in the sentence \"I think therefore I ____\", a bigram model might suggest \"am\" as the next word, based on previous observations.\n",
        "\n",
        "Autocomplete: Search engines use n-grams to provide search suggestions when you start typing a phrase. They are based on the prefixes of the current word and on the sentences written by the user.\n",
        "\n",
        "Text Generation: N-grams can be used to automatically generate text. They help a model predict next words based on the probabilities of words that can be generated depending on the context.\n",
        "\n",
        "You will also explore a way to assess the quality of generative language models using perplexity. Perplexity is a metric that evaluates the confidence with which a model is able to predict a sequence of words in text. The lower the perplexity, the more accurately the model is able to predict. That is, perplexity is a measure of how “surprised” a model is when exposed to a new set of words. We will use this metric to evaluate our n-gram based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEDULDRiRf_o"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1. Loading and pre-processing data\n",
        "\n",
        "<a name='1.1'></a>\n",
        "### 1.1 Loading data\n",
        "\n",
        "The data you will use in this work is contained in the file [trump.txt](./trump.txt)\n",
        "\n",
        "Read the contents of this file and store it in a data variable.\n",
        "\n",
        "Then display the first 300 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l8zdLKKARf_p",
        "outputId": "bf0a4284-42e1-4e1f-995f-09837e90a4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you very much.\n",
            "We had an amazing convention.\n",
            "That was one of the best.\n",
            "I think it was one of the best ever.\n",
            "In terms -- in terms of enthusiasm, in terms of I think what it represents, getting our word out.\n",
            "Ivanka was incredible last night.\n",
            "She did an incredible job.\n",
            "And so many of the speakers\n"
          ]
        }
      ],
      "source": [
        "\n",
        "filename = \"trump.txt\"\n",
        "with open(filename, encoding=\"utf8\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "print(data[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veFjXc6GRf_q"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 Segmentation\n",
        "\n",
        "Pre-process the data using the following steps:\n",
        "\n",
        "1. Remove capital letters.\n",
        "2. Replace “\\n” with spaces\n",
        "3. Separate the data into sentences using the following delimiters `.`, `?` and `!` as a separator.\n",
        "4. Remove punctuation marks (Be careful to keep spaces).\n",
        "5. Remove empty sentences.\n",
        "6. Segment sentences with the nltk.word_tokenize() function\n",
        "\n",
        "Then use your function to preprocess the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jU8f0VY5Rf_q"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def preprocess(data):\n",
        "\n",
        "    # Remove capital letters\n",
        "    data = data.lower()\n",
        "\n",
        "    # Remove line breaks\n",
        "    data = data.replace('\\n', ' ')\n",
        "\n",
        "    # Split into sentences using ., !, ? as delimiters\n",
        "    data = re.split('[.!?]', data)\n",
        "\n",
        "    # Remove punctuation\n",
        "    data = [re.sub(r'[^\\w\\s]', '', s) for s in data]\n",
        "\n",
        "    # Remove empty sentences\n",
        "    data = [sentence for sentence in data if sentence != '']\n",
        "\n",
        "    # Tokenize each sentence into words\n",
        "    data = [nltk.word_tokenize(sentence) for sentence in data]\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l6HaSkcYRf_q",
        "outputId": "c4907351-fac6-499e-b114-e357044b59f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "x = \"Cats are independent.\\nDogs are faithful.\"\n",
        "preprocess(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb25BI9NRf_r"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 Creating training and testing sets\n",
        "\n",
        "**Randomly** sample 80% of the data for the training set. Keep 20% for the test set. Use sklearn's train_test_split function. Store results in variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xj6_wtHORf_r"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = preprocess(data)\n",
        "\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im6I5qeRRf_r"
      },
      "source": [
        "<a name='1.4'></a>\n",
        "### 1.4 Vocabulary construction\n",
        "\n",
        "As in TP1, build a vocabulary from the training data. You can take your code from TP1.\n",
        "\n",
        "Complete the **build_voc** function which returns a list of tokens that are present at least n times (threshold passed as a parameter) in the list of examples (also passed as a parameter). You can use the collections.Counter class.\n",
        "\n",
        "Then call this function to build your vocabulary from the training set **using threshold=2**. Print the vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TZUdW6E2Rf_s"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_voc(documents, threshold = 0):\n",
        "\n",
        "    tokens = [token for sentence in documents for token in sentence]\n",
        "\n",
        "    counter = Counter(tokens)\n",
        "\n",
        "    vocabulary = [token for token, count in counter .items() if count >= threshold]\n",
        "    \n",
        "    return vocabulary + ['<e>']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de mots dans le vocabulaire (threshold de 2): 5609\n"
          ]
        }
      ],
      "source": [
        "X_train_vocabulary = build_voc(X_train, 2)\n",
        "print('Nombre de mots dans le vocabulaire (threshold de 2): {}'.format(len(X_train_vocabulary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCQp7HsfRf_s"
      },
      "source": [
        "<a name='1.5'></a>\n",
        "### 1.5 Non-vocabulary words\n",
        "\n",
        "If your model performs autocompletion, but it encounters a word that it has never seen during training, the model will not be able to predict the next word because there is no occurrence for the current word.\n",
        "\n",
        "These words are called Out of Vocabulary <b>OOV</b> words.\n",
        "The percentage of unknown words in the test set is called the <b>OOV</b> word rate.\n",
        "\n",
        "To handle unknown words when predicting, use a special 'unk' token to represent all unknown words. More specifically, the technique you will use will be the following:\n",
        "\n",
        "Complete the replace_oov function which converts all non-vocabulary words to the '\\<unk\\>' token.\n",
        "\n",
        "Then call your function on your training and testing corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "747vkEklRf_s"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def replace_oov(tokenized_sentences, voc, show_stats = True):\n",
        "\n",
        "    new_tokenized_sentences = []\n",
        "    sentences_copy = copy.deepcopy(tokenized_sentences) # Avoid modifying the original list\n",
        "\n",
        "    word_replace_count = 0\n",
        "    for sentence in sentences_copy:\n",
        "        for i, token in enumerate(sentence):\n",
        "            if token not in voc:\n",
        "                sentence[i] = '<unk>'\n",
        "                word_replace_count += 1\n",
        "        new_tokenized_sentences.append(sentence)\n",
        "\n",
        "    if show_stats:\n",
        "        print('OOV words replaced: {}'.format(word_replace_count))\n",
        "    \n",
        "    return new_tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mukYZoIMRf_s",
        "outputId": "3fbbe80a-f1e0-4dec-966c-359de1fcba23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOV words replaced: 3\n",
            "Phrase initiale:\n",
            "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
            "Phrase segmentée avec'<unk>':\n",
            "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]]\n",
        "vocabulary = build_voc([[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]], 2)\n",
        "tmp_replaced_tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
        "print(f\"Phrase initiale:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"Phrase segmentée avec'<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOV words replaced: 2738\n",
            "OOV words replaced: 1239\n"
          ]
        }
      ],
      "source": [
        "X_train = replace_oov(X_train, X_train_vocabulary)\n",
        "X_test = replace_oov(X_test, X_train_vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrFykMnSRf_s"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2. N-gram language patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv7Kv-zMRf_s"
      },
      "source": [
        "In this section, you will develop an n-grams language model. We will use the formula:\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{ t-1}\\dots w_{t-n})} \\tag{2} $$\n",
        "\n",
        "- The $C(\\cdots)$ function represents the number of occurrences of the given sequence.\n",
        "- $\\hat{P}$ means the estimate of $P$.\n",
        "\n",
        "You can estimate this probability by counting the occurrences of these word sequences in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyrhbEZZRf_s"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Frequency of n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oMVV0AERf_s"
      },
      "source": [
        "You will start by implementing a function that calculates the frequency of n-grams for an arbitrary number $n$.\n",
        "\n",
        "You need to pre-process the sentence by adding $n$ start-of-sentence markers \"\\<s\\>\" to indicate the start of the sentence.\n",
        "\n",
        "- For example, in a bigram model (N=2), the sequence should start with two starting tokens \"\\<s\\>\\<s\\>\". So, if the sentence is \"I like food\", change it to \"\\<s\\>\\<s\\> I like food\".\n",
        "- Also add an ending token \"\\<e\\>\" so the model can predict when to end a sentence.\n",
        "    \n",
        "\n",
        "In this implementation, you need to store the occurrences of n-grams as a dictionary.\n",
        "\n",
        "- The key of each key-value pair in the dictionary is a tuple of n words (not a list).\n",
        "- The value in the key-value pair is the number of occurrences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wxO80wyIRf_s"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "\n",
        "    n_grams = {}\n",
        "\n",
        "    for sentence in data:\n",
        "        # Add start and end tokens\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        # Create n-grams\n",
        "        n_grams_in_sentence = zip(*[sentence[i:] for i in range(n)])\n",
        "\n",
        "        # Count n-grams\n",
        "        for n_gram in n_grams_in_sentence:\n",
        "            if n_gram in n_grams:\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OAepnocxRf_s",
        "outputId": "90fe4ba3-0a97-4fa2-8954-d143b708c680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigrammes:\n",
            "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
            "Bigrammes:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "print(\"Unigrammes:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bigrammes:\")\n",
        "print(count_n_grams(sentences, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr1m0xETRf_t"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 MLE maximum likelihood estimate\n",
        "\n",
        "#### 2.2.1 Calculation of probability for a word\n",
        "\n",
        "\n",
        "Then estimate the probability of a word given the previous 'n' words with the frequencies obtained.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{ t-1}\\dots w_{t-n})} \\tag{2}$$\n",
        "\n",
        "\n",
        "The function takes as input:\n",
        "\n",
        "- word: the word whose probability we want to estimate\n",
        "- previous_n_gram: the previous n-gram, in tuple form\n",
        "- n_gram_counts: A dictionary where the key is the n-gram and the value is the frequency of that n-gram.\n",
        "- n_plus1_gram_counts: Another dictionary, which you will use to find the frequency of the previous n-gram plus the current word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DvGEdGeeRf_t"
      },
      "outputs": [],
      "source": [
        "\n",
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts):\n",
        "\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
        "\n",
        "    probability = n_plus1_gram_count / previous_n_gram_count\n",
        "\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKbv0PH0Rf_t"
      },
      "source": [
        "#### 2.2.1 What's wrong with this function? What pitfall could we encounter? Reply with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1EmNzj7Rf_t"
      },
      "source": [
        "> The probability of a sentence containing an N-gram never observed will always be 0.\n",
        "For example, with the corpus [['i', 'have', 'a', 'mouse'],\n",
        "              ['this', 'mouse', 'likes', 'cats']], if we wanted to estimate the probability of the word 'the', it will return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JZrTf6Rf_t"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 Add-k smoothing\n",
        "\n",
        "You will now modify your previous function using add-k smoothing.\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C( w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
        "\n",
        "Recode the function in number 2.2 by adding a smoothing constant $k$ and the vocabulary size as additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3pq6VJUKRf_t"
      },
      "outputs": [],
      "source": [
        "\n",
        "def estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "    denominator = previous_n_gram_count + k*vocabulary_size\n",
        "\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
        "\n",
        "    numerator = n_plus1_gram_count + k\n",
        "\n",
        "    probability = numerator / denominator\n",
        "\n",
        "    return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ybrCqIRgRf_t",
        "outputId": "0c98fb5f-18a3-4c83-be03-4e72acc5fec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "tmp_prob = estimate_probability_smoothing(\"have\", ['<s>', 'i'], bigram_counts, trigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\" La probabilité de 'have' étant donné le mot précédent 'i' est: {tmp_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NvWbviCRf_t"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 Calculation of probabilities of n-grams\n",
        "\n",
        "#### 2.4.1. Estimation of probabilities\n",
        "Complete the estimate_probabilities function which calculates for each word in the vocabulary the probability of being generated using the add-k smoothing function.\n",
        "\n",
        "Don't forget to add the special token \"\\<e\\>\" to the vocabulary\n",
        "\n",
        "This function takes as input:\n",
        "- previous_n_gram: the previous n-gram, in tuple form\n",
        "- n_gram_counts: A dictionary where the key is the n-gram and the value is the frequency of that n-gram.\n",
        "- n_plus1_gram_counts: Another dictionary, which you will use to find the frequency of the previous n-gram plus the current word.\n",
        "- vocabulary: vocabulary\n",
        "- k: the smoothing constant\n",
        "\n",
        "The function returns a dictionary having as keys all the words in the vocabulary as well as their probability of being generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XonXQIIqRf_t"
      },
      "outputs": [],
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k=k)\n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ijCjOzumRf_u",
        "outputId": "fd38ab0c-8e8f-46ec-a960-729dd9546fc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'i': 0.1111111111111111,\n",
              " 'have': 0.1111111111111111,\n",
              " 'a': 0.1111111111111111,\n",
              " 'mouse': 0.2222222222222222,\n",
              " 'this': 0.1111111111111111,\n",
              " 'likes': 0.1111111111111111,\n",
              " 'cats': 0.1111111111111111,\n",
              " '<e>': 0.1111111111111111}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = build_voc(sentences, threshold=0)\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edXYBYZsRf_u"
      },
      "source": [
        "#### 2.4.2. Probabilities given a context\n",
        "\n",
        "Now display the probabilities of tri-grams given the context \"i will\" using the training data. Show only the 10 most likely words in descending order of probability. Use K=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_unigrams = count_n_grams(X_train, 1)\n",
        "X_train_bigrams = count_n_grams(X_train, 2)\n",
        "X_train_trigrams = count_n_grams(X_train, 3)\n",
        "X_train_quadgrams = count_n_grams(X_train, 4)\n",
        "X_train_quintgrams = count_n_grams(X_train, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aQmw2lajRf_u",
        "outputId": "2c686218-56c7-4226-c795-7386a7c97b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilités des tri-grammes étant donné le context 'i will':\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('tell', 0.008267251560654632),\n",
              " ('fight', 0.005567740846971487),\n",
              " ('fix', 0.005230302007761093),\n",
              " ('be', 0.004555424329340307),\n",
              " ('never', 0.004049266070524717),\n",
              " ('say', 0.0035431078117091276),\n",
              " ('not', 0.002193352454867555),\n",
              " ('ask', 0.0020246330352623586),\n",
              " ('also', 0.0016871941960519656),\n",
              " ('work', 0.0016871941960519656)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Probabilités des tri-grammes étant donné le context 'i will':\")\n",
        "i_will_prob = estimate_probabilities(['i', 'will'], X_train_bigrams, X_train_trigrams, X_train_vocabulary, k=1)\n",
        "i_will_prob_top = sorted(i_will_prob.items(), key=lambda x: x[1], reverse=True)\n",
        "i_will_prob_top[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uCN6wl1Rf_y"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3. Perplexity\n",
        "\n",
        "In this section, you will generate the perplexity score to evaluate your model on the test set.\n",
        "\n",
        "To calculate the perplexity score of a sentence on an n-gram model, use:\n",
        "\n",
        "$$PP(W) =\\sqrt[N]{ \\prod_{t=1}^{N} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\ tag{4.1}$$\n",
        "\n",
        "where N = the number of tokens in the sentence including the token \\<e\\>\n",
        "and P = the probability of generating the token $w_t$\n",
        "\n",
        "The higher the probabilities, the lower the perplexity will be.\n",
        "\n",
        "<a name='3.1'></a>\n",
        "### 3.1. Calculation of perplexity\n",
        "Complete the `calculate_perplexity` function, which for a given sentence gives us the perplexity score. This function takes as input:\n",
        "\n",
        "\n",
        "- sentence: The sentence for which you must calculate the perplexity\n",
        "- n_gram_counts: A dictionary where the key is the n-gram and the value is the frequency of that n-gram.\n",
        "- n_plus1_gram_counts: Another dictionary, which you will use to find the frequency of the previous n-gram plus the current word.\n",
        "- vocabulary_size: the size of the vocabulary\n",
        "- k: the smoothing constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pV9dC6cmRf_y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "    # Calculate perplexity for one sentence\n",
        "    n = len(list(n_gram_counts.keys())[0]) # n is the n of the n-gram we use to estimate the probabilities\n",
        "\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    N = len(sentence) - n\n",
        "\n",
        "    product_pi = 1.0\n",
        "\n",
        "    for t in range(n, N+n):\n",
        "        word = sentence[t]\n",
        "        previous_n_gram = sentence[t-n:t]\n",
        "        probability = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
        "        product_pi *= 1/probability\n",
        "\n",
        "    perplexity = product_pi**(1.0/float(N))\n",
        "\n",
        "    return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n2VLK6ytRf_y",
        "outputId": "7b127463-47db-47eb-a4c3-2f2ed4b648fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité de la première phrase: 4.1930\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "\n",
        "perplexity = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexité de la première phrase: {perplexity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ8dUp2wRf_y"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2. Perplexity on a training sentence\n",
        "Calculate and display the perplexity of bi-gram, tri-gram and quad-gram models using your `calculate_perplexity` function defined above on the first sentence of your training corpus. Use K=0.01 here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité de la première phrase (bi-gramme): 49.2881\n",
            "Perplexité de la première phrase (tri-gramme): 29.3860\n",
            "Perplexité de la première phrase (quadri-gramme): 34.5169\n"
          ]
        }
      ],
      "source": [
        "perplexity_bigram = calculate_perplexity(X_train[0], \n",
        "                                         X_train_unigrams, X_train_bigrams, \n",
        "                                         len(X_train_vocabulary), k=0.01)\n",
        "\n",
        "perplexity_trigram = calculate_perplexity(X_train[0],\n",
        "                                          X_train_bigrams, X_train_trigrams,\n",
        "                                          len(X_train_vocabulary), k=0.01)\n",
        "\n",
        "perplexity_quadrigram = calculate_perplexity(X_train[0],\n",
        "                                             X_train_trigrams, X_train_quadgrams,\n",
        "                                             len(X_train_vocabulary), k=0.01)\n",
        "\n",
        "print(f\"Perplexité de la première phrase (bi-gramme): {perplexity_bigram:.4f}\")\n",
        "print(f\"Perplexité de la première phrase (tri-gramme): {perplexity_trigram:.4f}\")\n",
        "print(f\"Perplexité de la première phrase (quadri-gramme): {perplexity_quadrigram:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXIyUEp7Rf_y"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3. Perplexity of the test corpus\n",
        "\n",
        "#### 3.3.1. You can now calculate and display the perplexity of bi-gram, tri-gram and quad-gram models on your test corpus. K=1 here. (4 points)\n",
        "\n",
        "To calculate the perplexity of a corpus of *m* sentences, simply follow the following formula:\n",
        "\n",
        "Let $N$ be the total number of tokens in the test corpus C and $N_i$ be the number of tokens in sentence i.\n",
        "\n",
        "$$Perplexity(C) = \\Big(\\frac{1}{P(s_1, ..., s_m)}\\Big)^{1/N}$$\n",
        "$$P(s_1, ..., s_m) = \\prod_{i=1}^{m} p(s_i)$$\n",
        "$$p(s_i) = \\prod_{t=1}^{N_i} \\hat{P}(w_t | w_{t-n} \\cdots w_{t-1})$$\n",
        "\n",
        "Since it is a multiplication of probabilities (located between 0 and 1), the product becomes zero very quickly. This is why it is more efficient to transform to a logarithmic space to transform multiplications into addition. This gives the following formula:\n",
        "\n",
        "$$LogPerplexity(C) = 2^{-\\frac{1}{N} \\sum_{k=1}^{m} log_{2} \\; p(s_k)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FQErns7URf_y"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity_corpus(corpus, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "\n",
        "    N = sum([len(sentence) + 1 for sentence in corpus])\n",
        "\n",
        "    n = len(list(n_gram_counts.keys())[0]) # n is the n of the n-gram we use to estimate the probabilities\n",
        "\n",
        "    log_sum = 0\n",
        "    for sentence in corpus:\n",
        "\n",
        "        sentence_perplexity = 1.0\n",
        "        sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
        "        sentence = tuple(sentence)\n",
        "\n",
        "        N_i = len(sentence) - n\n",
        "\n",
        "        for t in range(n, N_i+n):\n",
        "            word = sentence[t]\n",
        "            previous_n_gram = sentence[t-n:t]\n",
        "            probability = estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
        "            sentence_perplexity *= probability\n",
        "\n",
        "        if sentence_perplexity != 0:\n",
        "            log_sum += math.log2(sentence_perplexity)\n",
        "\n",
        "    perplexity = 2**(-log_sum/N)\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "18oxZKhkRf_y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du corpus de test: 7.7089\n"
          ]
        }
      ],
      "source": [
        "n_gram_counts = {('<s>', 'quick'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
        "n_plus1_gram_counts = { ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
        "\n",
        "train_corpus = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
        "n_gram_counts = {('<s>', '<s>'): 2, ('<s>', 'the'): 1, ('<s>', 'jumps'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', '<e>'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
        "n_plus1_gram_counts = {('<s>', '<s>', '<s>', ): 2, ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1,  ('<s>', '<s>', 'jumps', ): 1, ('<s>', 'jumps', 'over'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', '<e>'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
        "\n",
        "vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"<e>\"]\n",
        "\n",
        "test_corpus = [[\"the\", \"fox\"], [\"jumps\"]]\n",
        "\n",
        "# Complétez le calcul de la perplexité avec k=1\n",
        "perplexity_test = calculate_perplexity_corpus(test_corpus, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k=1.0)\n",
        "print(f\"Perplexité du corpus de test: {perplexity_test:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "96nlCwpIRf_z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du corpus de test (bi): 1175.7622\n",
            "Perplexité du corpus de test (tri): 1860.5794\n",
            "Perplexité du corpus de test (quadri): 2188.1338\n"
          ]
        }
      ],
      "source": [
        "# Calculez mainenant la perplexité de votre corpus de test\n",
        "\n",
        "X_test_perplexity_bigram = calculate_perplexity_corpus(X_test, X_train_bigrams, X_train_trigrams, len(X_train_vocabulary), k=1.0)\n",
        "X_test_perplexity_trigram = calculate_perplexity_corpus(X_test, X_train_trigrams, X_train_quadgrams, len(X_train_vocabulary), k=1.0)\n",
        "X_test_perplexity_quadgram = calculate_perplexity_corpus(X_test, X_train_quadgrams, X_train_quintgrams, len(X_train_vocabulary), k=1.0)\n",
        "\n",
        "print(f\"Perplexité du corpus de test (bi): {X_test_perplexity_bigram:.4f}\")\n",
        "print(f\"Perplexité du corpus de test (tri): {X_test_perplexity_trigram:.4f}\")\n",
        "print(f\"Perplexité du corpus de test (quadri): {X_test_perplexity_quadgram:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexité du corpus d'entrainement (bi): 827.3116\n",
            "Perplexité du corpus d'entrainement (tri): 1186.9642\n",
            "Perplexité du corpus d'entrainement (quadri): 1342.3060\n"
          ]
        }
      ],
      "source": [
        "X_train_perplexity_bigram = calculate_perplexity_corpus(X_train, X_train_bigrams, X_train_trigrams, len(X_train_vocabulary), k=1.0)\n",
        "X_train_perplexity_trigram = calculate_perplexity_corpus(X_train, X_train_trigrams, X_train_quadgrams, len(X_train_vocabulary), k=1.0)\n",
        "X_train_perplexity_quadgram = calculate_perplexity_corpus(X_train, X_train_quadgrams, X_train_quintgrams, len(X_train_vocabulary), k=1.0)\n",
        "\n",
        "print(f\"Perplexité du corpus d'entrainement (bi): {X_train_perplexity_bigram:.4f}\")\n",
        "print(f\"Perplexité du corpus d'entrainement (tri): {X_train_perplexity_trigram:.4f}\")\n",
        "print(f\"Perplexité du corpus d'entrainement (quadri): {X_train_perplexity_quadgram:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDmVEP8IRf_z"
      },
      "source": [
        "#### 3.3.2. The expected perplexities may seem counterintuitive. Compare them to the perplexities obtained on the training set for the same models. How do you explain these results and what is your conclusion?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lhsVeFrgRf_z"
      },
      "source": [
        "> Comparing the perplexities of the test and training corpora, we notice that the perplexity is much lower on the training data compared to the test data. This behavior is normal because we have already seen the n-grams on which the models are trained.\n",
        "<br>\n",
        "<br>\n",
        "What is counterintuitive is that the perplexity of the models increases with the size N of the N-gram. We seek to minimize perplexity in order to find the best model. We would therefore expect the quad-gram model to be better than the others as its context is broader.\n",
        "<br>\n",
        "Our hypothesis is that we do not have enough data to properly evaluate larger N-gram models.\n",
        "The size of the data should also increase with the size of the N-gram in order to estimate the probabilities of word sequences.\n",
        "We are evaluating sequences of longer words, so we need more data to observe more sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z46qfzZLRf_z"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4. Building an auto-completion model\n",
        "\n",
        "In this last part, you will use the n-gram models constructed in previous numbers in order to make an autocompletion model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPp9NZisRf_z"
      },
      "source": [
        "<a name='4.1'></a>\n",
        "### 4.1 Suggestion of a word from a prefix\n",
        "\n",
        "\n",
        "The first step will be to construct a function that suggests a word from the first characters entered by a user, considering a single type of n-gram.\n",
        "\n",
        "Complete the `suggest_word` function which calculates the probabilities for all possible next words and suggests the most likely word. As an additional constraint, the suggested word must start with the prefix passed as a parameter. Use your functions from number 2. (N-gram model of words) to make your predictions.\n",
        "\n",
        "This function takes as parameter:\n",
        "- previous_n_gram: the previous n-gram, in tuple form\n",
        "- n_gram_counts: A dictionary where the key is the n-gram and the value is the frequency of that n-gram.\n",
        "- n_plus1_gram_counts: Another dictionary, which you will use to find the frequency of the previous n-gram plus the current word.\n",
        "- vocabulary_size: the size of the vocabulary\n",
        "- k: the smoothing constant\n",
        "- prefix: The beginning of the word we want to predict\n",
        "\n",
        "It returns the most probable word with the associated probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wVgc3874Rf_z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, prefixe=\"\"):\n",
        "\n",
        "    n = len(list(n_gram_counts.keys())[0]) # n is the n of the n-gram we use to estimate the probabilities\n",
        "\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    \n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "\n",
        "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
        "    for word, prob in probabilities.items():\n",
        "        if prob > max_prob and word.startswith(prefixe):\n",
        "            max_prob = prob\n",
        "            suggestion = word\n",
        "\n",
        "    return suggestion, max_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "A5Wm5gEVRf_z",
        "outputId": "e9adf014-1328-44c1-b294-670611a93c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avec les mots précédents 'i have',\n",
            "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
            "\n",
            "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
            "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = build_voc(sentences, threshold=0) # Build the vocabulary with the build_voc function that also adds the <e> token\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"have\"]\n",
        "tmp_suggest1 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"avec les mots précédents 'i have',\\n\\t le mot suggéré est `{tmp_suggest1[0]}` avec la probabilité {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "tmp_starts_with = 'm'\n",
        "tmp_suggest2 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, prefixe=tmp_starts_with)\n",
        "print(f\"avec les mots précédents 'i have', et une suggestion qui commence par `{tmp_starts_with}`\\n\\t le mot suggéré est : `{tmp_suggest2[0]}` avec une probabilité de {tmp_suggest2[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGs12YbSRf_z"
      },
      "source": [
        "<a name='4.2'></a>\n",
        "### 4.2 Multiple suggestions\n",
        "\n",
        "In order to suggest several words to the user, one strategy that one can use is to return a set of words suggested by several types of n-gram models.\n",
        "\n",
        "Using the `suggest_word` function from the previous issue, complete the `get_suggestions` function which returns the suggestions from the n-gram models passed as a parameter. You will also need to remove duplicates in the suggestions if there are any, and order the list of suggestions starting with the word with the highest probability.\n",
        "\n",
        "The get_suggestions function takes as parameters:\n",
        "- previous_n_gram: the previous n-gram, in tuple form\n",
        "- n_gram_counts_list: a list of n-grams in the following order [unigrams, bigrams, trigrams, quadrigrams, ...]\n",
        "- vocabulary_size: the size of the vocabulary\n",
        "- k: the smoothing constant (between 0 and 1)\n",
        "- prefix: The beginning of the word we want to predict, \"\" if no prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PzRJvpkVRf_z"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, prefixe=\"\"):\n",
        "\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "\n",
        "    suggestions = set()\n",
        "    for i in range(model_counts - 1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        n = len(list(n_gram_counts.keys())[0]) # n is the n of the n-gram we use to estimate the probabilities\n",
        "\n",
        "        if len(previous_tokens) >= n: # Only evalute n-grams that don't exceed the length of the sentence. MWe are look at n-grams that come after the previous tokens.\n",
        "            suggestion = suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k, prefixe=prefixe)\n",
        "            suggestions.add(suggestion)\n",
        "\n",
        "    # Sort suggestions\n",
        "    suggestions = sorted(suggestions, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return only the words\n",
        "    suggestions = [tup[0] for tup in suggestions]\n",
        "\n",
        "    # keep only unique values\n",
        "    suggestions = list(set(suggestions))\n",
        "\n",
        "    return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BrBQhVp2Rf_0",
        "outputId": "70c35058-dfd8-4423-88b7-e48cff99339d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Etant donné les mots i have, je suggère :\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['a']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# test\n",
        "sentences = [['i', 'have', 'a', 'mouse'],\n",
        "             ['this', 'mouse', 'likes', 'cats']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "quadgram_counts = count_n_grams(sentences, 4)\n",
        "qintgram_counts = count_n_grams(sentences, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "previous_tokens = [\"i\", \"have\"]\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
        "\n",
        "print(f\"Etant donné les mots i have, je suggère :\")\n",
        "display(tmp_suggest3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ne2XeIeRf_0"
      },
      "source": [
        "<a name='4.3'></a>\n",
        "### 4.3 Autocompletion\n",
        "\n",
        "Now it's time to combine your functions to create the autocomplete template. Using the training dataset, calculate the frequency of n-grams ranging from 1 to 5 and use the *get_suggestions* function to suggest words. You will need to be able to always suggest words starting from the last word entered by the user.\n",
        "\n",
        "Complete the *update_suggestions* function:\n",
        "- the current_text variable contains all the text entered by the user\n",
        "- the top_suggestions variable contains the suggestions that will be offered\n",
        "\n",
        "You will need to change the contents of the top_suggestions variable so that it contains the n-gram suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SbjsHmhnRf_0"
      },
      "outputs": [],
      "source": [
        "X_train_n_gram_counts_list = [X_train_unigrams, X_train_bigrams, X_train_trigrams, X_train_quadgrams, X_train_quintgrams]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "cc4598dd67004ff9bdecc58db4499a1b",
            "4d2773b090254c4a8599abd14264f1d9"
          ]
        },
        "id": "lDJwHOgfRf_0",
        "outputId": "7d024a5b-eae8-4fd0-f6cd-28532956f8fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a46191744dac4bd48815e37a5633a8ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', placeholder='Entrez votre text ici...')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "766c2ba9752a42dfb3a97ba41128aee1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Label(value='Suggestions: ')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "text_input = widgets.Text(placeholder=\"Entrez votre text ici...\")\n",
        "\n",
        "suggestions_label = widgets.Label(value=\"Suggestions: \")\n",
        "\n",
        "def update_suggestions(change):\n",
        "     texte_actuel = change[\"new\"]\n",
        "\n",
        "     if texte_actuel != \"\":\n",
        "         # Tokenize the text\n",
        "         text_data = preprocess(texte_actuel)\n",
        "\n",
        "         # Replace out-of-vocabulary words\n",
        "         tokenized_text = replace_oov(text_data, X_train_vocabulary, False)\n",
        "\n",
        "         # Get suggestions\n",
        "         top_suggestions = get_suggestions(tokenized_text[0], X_train_n_gram_counts_list, X_train_vocabulary, k=1.0, prefixe=\"\")\n",
        "\n",
        "         suggestions_label.value = \"Suggestions: \" + \", \".join(top_suggestions)\n",
        "\n",
        "text_input.observe(update_suggestions, names=\"value\")\n",
        "\n",
        "display(text_input)\n",
        "display(suggestions_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBDpuZc5Rf_0"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Sentence generation model\n",
        "\n",
        "In this part you will build a sentence generation model using n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67xJK9rWRf_0"
      },
      "source": [
        "#### As part of a sentence generation model, indicate why the word suggestion strategy in 4. cannot work?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrud_s4wRf_0"
      },
      "source": [
        ">In question 4, we return the most probable word according to the n-gram. This is problematic in the context of sentence generation because we are starting from a non-existent context, so the same word will be judged most likely to start the sentence. So the first word will always be the same and the following words won't have much variety because they will all depend on the same context. An added stochastic effect could prevent this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBRTAM8BRf_0"
      },
      "source": [
        "<a name='5.1'></a>\n",
        "\n",
        "### 5.1 Stochastic generation of words\n",
        "\n",
        "Recode the suggest_word function to use a stochastic suggestion. In other words, instead of returning the most probable word, you will have to generate the next word according to its probability.\n",
        "\n",
        "For example if the word 'like' has probability 0.25 of being generated, then it will be returned 25% of the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rmgGGnjqRf_0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def suggest_word_with_probs(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    \n",
        "    n = len(list(n_gram_counts.keys())[0]) # n is the n of the n-gram we use to estimate the probabilities\n",
        "\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
        "    words = list(probabilities.keys())\n",
        "    probs = list(probabilities.values())\n",
        "    \n",
        "    chosen_word = random.choices(words, weights=probs, k=1)[0]\n",
        "    return chosen_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhcabjB9Rf_0"
      },
      "source": [
        "<a name='5.2'></a>\n",
        "### 5.2 Sentence generations\n",
        "\n",
        "#### 5.2.1. Stochastic generation\n",
        "Now complete the `generate_sentence` function which generates an n_words long sentence by calling your new function `suggest_words_with_probs`. Generation should stop if the model generates an end-of-sentence token.\n",
        "\n",
        "Don't forget to initialize the sentences to be generated with the right number of sentence start tokens (`<s>`). For example, if it is a bigram model, you will have to initialize the sentence at [`<s>`]. If it is a trigram model, you will have to initialize the sentence at [`<s>`, `<s>`]. You can find the context size using the following expression `len(next(iter(n_gram_counts)))`.\n",
        "\n",
        "Then, you will need to pass to the `suggest_word` function the last `n` words generated where `n` corresponds to the size of the context.\n",
        "Finally, you will have to stop the generation if the generated token is the end token (`<e>`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JE25q-yORf_0"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.0001):\n",
        "    n = len(next(iter(n_gram_counts)))\n",
        "    \n",
        "    sentence = ['<s>'] * n\n",
        "    \n",
        "    for _ in range(n_words):\n",
        "        next_word = suggest_word_with_probs(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
        "\n",
        "        if next_word == '<e>':\n",
        "            break\n",
        "            \n",
        "        sentence.append(next_word)\n",
        "        \n",
        "    return sentence[n:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4iY2tfPRf_0"
      },
      "source": [
        "#### 5.2.2. Test on n-grams\n",
        "Then test your function with trigrams and 5-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XBOApJfPRf_0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée pour trigramme: ['we', 'will', 'cancel', 'revived', 'rogue', 'tougher', 'hospital', 'instance', 'intelligently', 'radicalism', 'dudes', 'affect', 'mayors', 'drivers', 'excitement', 'trees', 'flag', '500', 'riley', 'jetfighters']\n"
          ]
        }
      ],
      "source": [
        "trigram_sentence = generate_sentence(10, X_train_trigrams, X_train_quadgrams, X_train_vocabulary)\n",
        "print(f\"Phrase générée pour trigramme: {trigram_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "gK90j9bMRf_1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée pour 5-gramme: ['ive', 'outlined', 'a', 'plan', 'to', 'provide', 'every', 'disadvantaged', 'child', 'franklin', 'break', 'kind', 'golfer', 'farms', 'renegotiate', 'authorize', 'truck', 'vanishing', 'post', 'cheap']\n"
          ]
        }
      ],
      "source": [
        "X_train_sixgrams = count_n_grams(X_train, 6)\n",
        "quintgram_sentence = generate_sentence(10, X_train_quintgrams, X_train_sixgrams, X_train_vocabulary)\n",
        "print(f\"Phrase générée pour 5-gramme: {quintgram_sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvAcYTmQRf_1"
      },
      "source": [
        "#### 5.2.3. With k=1.0, what happens to the generated sentences and what is the main reason? What can you do to improve the situation?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-4l6fhRf_1"
      },
      "source": [
        ">The generated sentences are no longer coherent.\n",
        "The smoothing is too strong and the probabilities are therefore falsely evaluated.\n",
        "<br>\n",
        "Words that should have low probabilities are rated higher and are suggested by the algorithm.\n",
        "<br>\n",
        "<br>\n",
        "It is therefore necessary to adjust the parameter K so that the model suggests coherent sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ekqrVURf_1"
      },
      "source": [
        "#### 5.2.4. What are the problems if the constant k has a value too small, see 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Wjlm83Rf_1"
      },
      "source": [
        ">With k=0, we find ourselves in a sort of over-fitting situation.\n",
        "<br>\n",
        "The probabilities of n-grams missing from the training data drop to 0.\n",
        "<br>\n",
        "Thus, only the n-grams seen in training are considered during the random selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxWS1OPRf_1"
      },
      "source": [
        "<a name='5.3'></a>\n",
        "### 5.3. Improved stochastic word generation\n",
        "\n",
        "#### 5.3.1. Stochastic improvement\n",
        "\n",
        "As you may have observed, stochastic generation, although effective at generating different sentences, tends not to generate consistently coherent sentences. Propose an improvement to the `suggest_word` method that you will implement in the `suggest_word_new` method to generate more coherent sentences.\n",
        "\n",
        "##### a) Describe your method in the following cell"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DAD2VNybRf_1"
      },
      "source": [
        ">suggest_word_with_probs gives a probability to out-of-context words with a very low chance. We can remove a degree of stochasticity by randomly drawing from the x most probable words according to a given context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCehUJU3Rf_1"
      },
      "source": [
        "##### b) Implement the proposed method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iTnupro6Rf_1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def suggest_word_new(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    x = 20 # we choose the top n words to keep and choose randomly between those\n",
        "    n = len(list(n_gram_counts.keys())[0]) # n is the n of the n-gram we use to estimate the probabilities\n",
        "\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
        "    sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:x]\n",
        "\n",
        "    words, probs = zip(*sorted_probs)\n",
        "    chosen_word = random.choices(words, weights=probs)[0]\n",
        "\n",
        "    return chosen_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bESshtAqRf_1"
      },
      "source": [
        "#### 5.3.2. Improved generation\n",
        "Now recode the `generate_sentence_new` function to call your new `suggest_word_new` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eSmfUVq5Rf_1"
      },
      "outputs": [],
      "source": [
        "def generate_sentence_new(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.001):\n",
        "    n = len(next(iter(n_gram_counts)))\n",
        "    \n",
        "    sentence = ['<s>'] * n\n",
        "        \n",
        "    for _ in range(n_words):\n",
        "        next_word = suggest_word_new(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
        "\n",
        "        if next_word == '<e>':\n",
        "            break\n",
        "\n",
        "        sentence.append(next_word)\n",
        "\n",
        "    return sentence[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetI7zQoRf_1"
      },
      "source": [
        "#### 5.3.3. Test on n-grams\n",
        "Then test your function with 3-grams and 5-grams and validate that the sentences are more coherent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LAN99kklRf_1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée pour le trigramme: ['the', 'great', 'state', 'of', 'pennsylvania']\n"
          ]
        }
      ],
      "source": [
        "trigram_sentence = generate_sentence_new(20, X_train_trigrams, X_train_quadgrams, X_train_vocabulary)\n",
        "print(f\"Phrase générée pour le trigramme: {trigram_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "shrPSXK3Rf_1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase générée pour le quintgramme: ['were', 'gon', 'na', 'build', 'it', 'up']\n"
          ]
        }
      ],
      "source": [
        "quintgram_sentence = generate_sentence_new(20, X_train_quintgrams, X_train_sixgrams, X_train_vocabulary)\n",
        "print(f\"Phrase générée pour le quintgramme: {quintgram_sentence}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "NLPC2-3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
